# EECS504_HW4
Backpropagation
In this problem, we will train a two-layer neural network to classify images. Our network will have two layers, and a softmax layer to perform classification. Weâ€™ll train the network to minimize a cross-entropy loss function (also known as softmax loss). The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:
1) input, 2) fully connected layer, 3) ReLU, 4) fully connected layer, 5) softmax.

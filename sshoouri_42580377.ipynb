{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ix5dQS2rUMlu"
   },
   "source": [
    "#EECS 504 PS4: Backpropagation\n",
    "\n",
    "Please provide the following information \n",
    "(e.g. Andrew Owens, ahowens):\n",
    "\n",
    "Sara Shoouri sshoouri\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_Cst4k4tuBc"
   },
   "source": [
    "# Starting\n",
    "\n",
    "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHumIO-xt57H"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision.datasets import CIFAR10\n",
    "download = not os.path.isdir('cifar-10-batches-py')\n",
    "dset_train = CIFAR10(root='.', download=download)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apEPzDNtK0MC"
   },
   "source": [
    "# Problem 4.2 Multi-layer perceptron\n",
    "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
    "\n",
    "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
    "\n",
    "input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "The outputs of the second fully-connected layer are the scores for each class.\n",
    "\n",
    "You cannot use any deep learning libraries such as PyTorch in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXfumCQ21JoK"
   },
   "source": [
    "# 4.2 (a) Layers\n",
    "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-ljfgMv9PHx"
   },
   "outputs": [],
   "source": [
    "def fc_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a fully-connected layer.\n",
    "    \n",
    "    The input x has shape (N, Din) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (Din,).\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, Din)\n",
    "    - w: A numpy array of weights, of shape (Din, Dout)\n",
    "    - b: A numpy array of biases, of shape (Dout,)\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, Dout)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the forward pass. Store the result in out.              #\n",
    "    ###########################################################################\n",
    "   \n",
    "    out=None;\n",
    "    out=x.dot(w)+b;\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def fc_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a fully_connected layer.\n",
    "    \n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, Dout)\n",
    "    - cache: returned by your forward function. Tuple of:\n",
    "      - x: Input data, of shape (N, Din)\n",
    "      - w: Weights, of shape (Din, Dout)\n",
    "      - b: Biases, of shape (Dout,)\n",
    "      \n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, Din)\n",
    "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
    "    - db: Gradient with respect to b, of shape (Dout,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "    dx = dout.dot(w.T);\n",
    "    dw = x.T.dot(dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx, dw, db\n",
    "\n",
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = x*(x>0)\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                  #\n",
    "    ###########################################################################\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: returned by your forward function. Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    x=cache;\n",
    "    dx = dout*(x>0);\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                 #\n",
    "    ###########################################################################\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx\n",
    "\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    loss, dx = None, None\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement softmax loss                                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    \n",
    "    expss = x - np.max(x, axis=1, keepdims=True)\n",
    "    sumf = np.sum(np.exp(expss), axis=1, keepdims=True)\n",
    "    softL = expss - np.log(sumf)\n",
    "    Hs = np.exp(softL)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(softL[np.arange(N), y]) / N\n",
    "    dx = Hs.copy()\n",
    "    dx[np.arange(N), y]=dx[np.arange(N), y]-1;\n",
    "    \n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return loss, dx/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbFxtS3zK8oz"
   },
   "source": [
    "# 4.2 (b) Softmax Classifier\n",
    "\n",
    "In this problem, implement softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytvxbx9UpxVL"
   },
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(object):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network with\n",
    "    softmax loss that uses a modular layer design. We assume an input dimension\n",
    "    of D, a hidden dimension of H, and perform classification over C classes.\n",
    "\n",
    "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
    "\n",
    "    The learnable parameters of the model are stored in the dictionary\n",
    "    self.params that maps parameter names to numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
    "                 weight_scale=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: An integer giving the size of the input\n",
    "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
    "          if there's no hidden layer.\n",
    "        - num_classes: An integer giving the number of classes to classify\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        ############################################################################\n",
    "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
    "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
    "        # standard deviation equal to weight_scale, and biases should be           #\n",
    "        # initialized to zero. All weights and biases should be stored in the      #\n",
    "        # dictionary self.params, with fc weights and biases using the keys        #\n",
    "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
    "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
    "        ############################################################################\n",
    "        self.hidden_dim = hidden_dim;\n",
    "        weighL1 = np.random.randn(input_dim, hidden_dim) * weight_scale; #normal” (Gaussian) distribution of mean 0 and variance 1\n",
    "        self.params['weighL1'] = weighL1;\n",
    "\n",
    "        bias1 = np.zeros(hidden_dim);\n",
    "        self.params['bias1'] = bias1;\n",
    "\n",
    "        weighL2 = np.random.randn(hidden_dim, num_classes) * weight_scale;\n",
    "        self.params['weighL2'] = weighL2;\n",
    "\n",
    "        bias2 = np.zeros(num_classes);\n",
    "        self.params['bias2'] = bias2;\n",
    "\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "    def forwards_backwards(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for a minibatch of data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, Din)\n",
    "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "\n",
    "        Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return:\n",
    "        - scores: Array of shape (N, C) giving classification scores, where\n",
    "          scores[i, c] is the classification score for X[i] and class c.\n",
    "\n",
    "        If y is not None, then run a training-time forward and backward pass. And\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "          names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        scores = None\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
    "        # class scores for X and storing them in the scores variable.              #\n",
    "        ############################################################################\n",
    "        firstLayer, cache1 = fc_forward(X, self.params['weighL1'], self.params['bias1']);\n",
    "        fX,cacheX=relu_forward(firstLayer);\n",
    "        scores, cache2 = fc_forward(fX, self.params['weighL2'], self.params['bias2']);\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        # If y is None then we are in test mode so just return scores\n",
    "        if y is None:\n",
    "            return scores\n",
    "        loss, grads = 0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
    "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
    "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
    "        # self.params[k].                                                          # \n",
    "        ############################################################################\n",
    "        loss, d_loss = softmax_loss(scores, y);\n",
    "        dx2, dw2, db2 = fc_backward(d_loss, cache2)\n",
    "        dx15 = relu_backward(dx2, cacheX)\n",
    "        dx1, dw1, db1 = fc_backward(dx15, cache1)\n",
    "        grads['weighL1'] = dw1;\n",
    "        grads['bias1'] = db1;\n",
    "        grads['weighL2'] = dw2;\n",
    "        grads['bias2'] = db2;\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "        return loss, grads\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwp0waIL1h_e"
   },
   "source": [
    "# 4.2(c) Training\n",
    "\n",
    "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "kZPtQzXGMoCg",
    "outputId": "d7845f87-8660-4f03-da01-44abfd381b3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40000) loss: 2.322315\n",
      "(Epoch 0 / 10) train acc: 0.109000; val_acc: 0.084900\n",
      "(Iteration 1001 / 40000) loss: 2.172486\n",
      "(Iteration 2001 / 40000) loss: 1.967210\n",
      "(Iteration 3001 / 40000) loss: 2.282813\n",
      "(Epoch 1 / 10) train acc: 0.346000; val_acc: 0.325400\n",
      "(Iteration 4001 / 40000) loss: 1.708562\n",
      "(Iteration 5001 / 40000) loss: 1.742310\n",
      "(Iteration 6001 / 40000) loss: 1.868380\n",
      "(Iteration 7001 / 40000) loss: 1.905705\n",
      "(Epoch 2 / 10) train acc: 0.388000; val_acc: 0.369200\n",
      "(Iteration 8001 / 40000) loss: 1.608013\n",
      "(Iteration 9001 / 40000) loss: 2.326361\n",
      "(Iteration 10001 / 40000) loss: 1.890874\n",
      "(Iteration 11001 / 40000) loss: 1.718404\n",
      "(Epoch 3 / 10) train acc: 0.399000; val_acc: 0.394200\n",
      "(Iteration 12001 / 40000) loss: 1.719530\n",
      "(Iteration 13001 / 40000) loss: 1.798931\n",
      "(Iteration 14001 / 40000) loss: 1.784529\n",
      "(Iteration 15001 / 40000) loss: 1.765307\n",
      "(Epoch 4 / 10) train acc: 0.423000; val_acc: 0.411600\n",
      "(Iteration 16001 / 40000) loss: 1.498585\n",
      "(Iteration 17001 / 40000) loss: 1.544272\n",
      "(Iteration 18001 / 40000) loss: 1.392165\n",
      "(Iteration 19001 / 40000) loss: 1.261001\n",
      "(Epoch 5 / 10) train acc: 0.449000; val_acc: 0.422100\n",
      "(Iteration 20001 / 40000) loss: 1.665738\n",
      "(Iteration 21001 / 40000) loss: 1.486582\n",
      "(Iteration 22001 / 40000) loss: 1.589233\n",
      "(Iteration 23001 / 40000) loss: 1.792257\n",
      "(Epoch 6 / 10) train acc: 0.446000; val_acc: 0.437100\n",
      "(Iteration 24001 / 40000) loss: 1.345073\n",
      "(Iteration 25001 / 40000) loss: 1.645817\n",
      "(Iteration 26001 / 40000) loss: 1.087421\n",
      "(Iteration 27001 / 40000) loss: 1.767728\n",
      "(Epoch 7 / 10) train acc: 0.496000; val_acc: 0.443600\n",
      "(Iteration 28001 / 40000) loss: 2.061478\n",
      "(Iteration 29001 / 40000) loss: 1.359170\n",
      "(Iteration 30001 / 40000) loss: 1.969356\n",
      "(Iteration 31001 / 40000) loss: 1.340718\n",
      "(Epoch 8 / 10) train acc: 0.472000; val_acc: 0.448400\n",
      "(Iteration 32001 / 40000) loss: 0.915784\n",
      "(Iteration 33001 / 40000) loss: 1.603732\n",
      "(Iteration 34001 / 40000) loss: 1.597670\n",
      "(Iteration 35001 / 40000) loss: 2.037880\n",
      "(Epoch 9 / 10) train acc: 0.490000; val_acc: 0.454000\n",
      "(Iteration 36001 / 40000) loss: 2.106536\n",
      "(Iteration 37001 / 40000) loss: 1.464794\n",
      "(Iteration 38001 / 40000) loss: 1.263453\n",
      "(Iteration 39001 / 40000) loss: 1.247423\n",
      "(Epoch 10 / 10) train acc: 0.502000; val_acc: 0.461500\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding=\"latin1\")\n",
    "    return dict\n",
    "\n",
    "def load_cifar10():\n",
    "    data = {}\n",
    "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
    "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
    "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
    "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
    "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
    "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
    "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
    "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
    "                         batch4['data'], batch5['data']))\n",
    "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
    "                       batch4['labels'] + batch5['labels'])\n",
    "    X_test = test_batch['data']\n",
    "    Y_test = test_batch['labels']\n",
    "    \n",
    "    #Preprocess images here                                     \n",
    "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
    "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
    "\n",
    "    data['X_train'] = X_train[:40000]\n",
    "    data['y_train'] = Y_train[:40000]\n",
    "    data['X_val'] = X_train[40000:]\n",
    "    data['y_val'] = Y_train[40000:]\n",
    "    data['X_test'] = X_test\n",
    "    data['y_test'] = Y_test\n",
    "    return data\n",
    "\n",
    "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
    "    \"\"\"\n",
    "    Check accuracy of the model on the provided data.\n",
    "\n",
    "    Inputs:\n",
    "    - model: Image classifier\n",
    "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
    "    - y: Array of labels, of shape (N,)\n",
    "    - num_samples: If not None, subsample the data and only test the model\n",
    "      on num_samples datapoints.\n",
    "    - batch_size: Split X and y into batches of this size to avoid using\n",
    "      too much memory.\n",
    "\n",
    "    Returns:\n",
    "    - acc: Scalar giving the fraction of instances that were correctly\n",
    "      classified by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Subsample the data\n",
    "    N = X.shape[0]\n",
    "    if num_samples is not None and N > num_samples:\n",
    "        mask = np.random.choice(N, num_samples)\n",
    "        N = num_samples\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "    # Compute predictions in batches\n",
    "    num_batches = N // batch_size\n",
    "    if N % batch_size != 0:\n",
    "        num_batches += 1\n",
    "    y_pred = []\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        scores = model.forwards_backwards(X[start:end])\n",
    "        y_pred.append(np.argmax(scores, axis=1))\n",
    "    y_pred = np.hstack(y_pred)\n",
    "    acc = np.mean(y_pred == y)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_network(model, data, **kwargs):\n",
    "    \"\"\"\n",
    "     Required arguments:\n",
    "    - model: Image classifier\n",
    "    - data: A dictionary of training and validation data containing:\n",
    "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
    "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
    "      'y_train': Array, shape (N_train,) of labels for training images\n",
    "      'y_val': Array, shape (N_val,) of labels for validation images\n",
    "\n",
    "    Optional arguments:\n",
    "    - learning_rate: A scalar for initial learning rate.\n",
    "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
    "      learning rate is multiplied by this value.\n",
    "    - batch_size: Size of minibatches used to compute loss and gradient\n",
    "      during training.\n",
    "    - num_epochs: The number of epochs to run for during training.\n",
    "    - print_every: Integer; training losses will be printed every\n",
    "      print_every iterations.\n",
    "    - verbose: Boolean; if set to false then no output will be printed\n",
    "      during training.\n",
    "    - num_train_samples: Number of training samples used to check training\n",
    "      accuracy; default is 1000; set to None to use entire training set.\n",
    "    - num_val_samples: Number of validation samples to use to check val\n",
    "      accuracy; default is None, which uses the entire validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
    "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
    "    batch_size = kwargs.pop('batch_size', 100)\n",
    "    num_epochs = kwargs.pop('num_epochs', 10)\n",
    "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
    "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
    "    print_every = kwargs.pop('print_every', 10)   \n",
    "    verbose = kwargs.pop('verbose', True)\n",
    "    \n",
    "    epoch = 0\n",
    "    best_val_acc = 0\n",
    "    best_params = {}\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    \n",
    "    num_train = data['X_train'].shape[0]\n",
    "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
    "    num_iterations = num_epochs * iterations_per_epoch\n",
    "    \n",
    "\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Make a minibatch of training data\n",
    "        batch_mask = np.random.choice(num_train, batch_size)\n",
    "        X_batch = data['X_train'][batch_mask]\n",
    "        y_batch = data['y_train'][batch_mask]\n",
    "        \n",
    "        # Compute loss and gradient\n",
    "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Perform a parameter update\n",
    "        for p, w in model.params.items():\n",
    "            model.params[p] = w - grads[p]*learning_rate\n",
    "          \n",
    "        # Print training loss\n",
    "        if verbose and t % print_every == 0:\n",
    "            print('(Iteration %d / %d) loss: %f' % (\n",
    "                   t + 1, num_iterations, loss_history[-1]))\n",
    "         \n",
    "        # At the end of every epoch, increment the epoch counter and decay\n",
    "        # the learning rate.\n",
    "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "        if epoch_end:\n",
    "            epoch += 1\n",
    "            learning_rate *= lr_decay\n",
    "        \n",
    "        # Check train and val accuracy on the first iteration, the last\n",
    "        # iteration, and at the end of each epoch.\n",
    "        first_it = (t == 0)\n",
    "        last_it = (t == num_iterations - 1)\n",
    "        if first_it or last_it or epoch_end:\n",
    "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
    "                num_samples= num_train_samples)\n",
    "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
    "                num_samples=num_val_samples)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "\n",
    "            if verbose:\n",
    "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
    "                       epoch, num_epochs, train_acc, val_acc))\n",
    "\n",
    "            # Keep track of the best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_params = {}\n",
    "                for k, v in model.params.items():\n",
    "                    best_params[k] = v.copy()\n",
    "        \n",
    "    model.params = best_params\n",
    "        \n",
    "    return model, train_acc_history, val_acc_history\n",
    "        \n",
    "\n",
    "# load data\n",
    "data = load_cifar10() \n",
    "train_data = { k: data[k] for k in ['X_train','y_train','X_val','y_val']}\n",
    "#######################################################################\n",
    "# TODO: Set up model hyperparameters                                  #\n",
    "#######################################################################\n",
    "\n",
    "# initialize model\n",
    "model = SoftmaxClassifier(hidden_dim =150, weight_scale=1e-2)\n",
    "\n",
    "# start training    \n",
    "model, train_acc_history, val_acc_history = train_network(\n",
    "    model, train_data, learning_rate =0.007 ,\n",
    "    lr_decay=0.95, num_epochs=10, \n",
    "    batch_size=10, print_every=1000)\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcovGmpXvXXa"
   },
   "source": [
    "# 4.2(c) Report Accuracy\n",
    "\n",
    "Run the given code and report the accuracy on test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FwCq8pBhu6dz",
    "outputId": "f6eec834-5cbe-482a-ba1b-8bbf0c3f3239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.4651\n"
     ]
    }
   ],
   "source": [
    "# report test accuracy\n",
    "acc = test_network(model, data['X_test'], data['y_test'])\n",
    "print(\"Test accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTrmbULS7i2N"
   },
   "source": [
    "# 4.2(d) Plot\n",
    "\n",
    "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "SPjtnbya9S7g",
    "outputId": "c514a918-e498-4bbc-c2a3-cf57c5c0c093"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEGCAYAAACpXNjrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEUW9cNLxtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax1 = plt.subplots();\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.plot(range(11), train_acc_history,color='red');\n",
    "ax2 = ax1.twinx();\n",
    "ax2.plot(range(11), val_acc_history,color='blue');\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of EECS 504 PS4: Backpropagation",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
